{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(batch):\n",
    "    # print(len(batch[\"text\"]))\n",
    "    input_tensors = []\n",
    "    mask_tensors = []\n",
    "    for text in batch[\"text\"]:\n",
    "        tweet = tokenizer(text, return_tensors=\"pt\", padding=\"max_length\",truncation=True,max_length=256)\n",
    "        input_ids, attention_mask = tweet[\"input_ids\"], tweet[\"attention_mask\"]\n",
    "        input_tensors.append(input_ids)\n",
    "        mask_tensors.append(attention_mask)\n",
    "    # print(len(batch[\"text\"]))\n",
    "    input_ids, attention_mask = torch.stack(input_tensors, dim=0), torch.stack(mask_tensors, dim=0)\n",
    "    # print(input_ids.shape)\n",
    "    prices = torch.tensor(batch[\"open\"]).unsqueeze(-1).float()\n",
    "    volumes = torch.tensor(batch[\"volume\"]).unsqueeze(-1).float()\n",
    "    labels = torch.tensor(batch[\"close\"]).float()\n",
    "\n",
    "    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"prices\": prices, \"volumes\": volumes, \"labels\": labels}\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    input_ids = [torch.tensor(item[\"input_ids\"]) for item in batch]\n",
    "    attention_mask = [torch.tensor(item[\"attention_mask\"]) for item in batch]\n",
    "    prices = [torch.tensor(item[\"prices\"]) for item in batch]\n",
    "    volumes = [torch.tensor(item[\"volumes\"]) for item in batch]\n",
    "    labels = [torch.tensor(item[\"labels\"]) for item in batch]\n",
    "\n",
    "    input_ids = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    attention_mask = torch.nn.utils.rnn.pad_sequence(attention_mask, batch_first=True, padding_value=0)\n",
    "    prices = torch.nn.utils.rnn.pad_sequence(prices, batch_first=True, padding_value=0)\n",
    "    volumes = torch.nn.utils.rnn.pad_sequence(volumes, batch_first=True, padding_value=0)\n",
    "    labels = torch.stack(labels)\n",
    "\n",
    "    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"prices\": prices, \"volumes\": volumes, \"labels\": labels}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/Users/pan/.cache/huggingface/datasets/json/default-b509911a125022b9/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['name', 'date', 'open', 'high', 'low', 'close', 'volume', 'adj_close', 'text', 'label'],\n",
      "    num_rows: 19059\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "hf_dataset = load_dataset(\"json\", data_files={\"train\": \"/Users/pan/Documents/course/EECS545/Group/Stock-Market-Prediction/data/tweet_price/aligned_data.json\"}, split=\"train\")\n",
    "print(hf_dataset)\n",
    "# stat = []\n",
    "# for i in range(10):\n",
    "#     stat.append(len(\" \".join(hf_dataset[i][\"text\"])))\n",
    "# print(max(stat))\n",
    "# print(min(stat))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['name', 'date', 'open', 'high', 'low', 'close', 'volume', 'adj_close', 'text', 'label', 'input_ids', 'attention_mask', 'prices', 'volumes', 'labels'],\n",
      "    num_rows: 19059\n",
      "})\n",
      "['weekly dow stocks trend $ dis $ wmt $ hd $ gs $ v $ intc $ ibm $ utx $ vz $ unh $ t $ msft $ axp $ jpm $ mrk $ csco $ ko $ cvx @ URL $ vz - a new year means time for new dogs of the dow -> URL stock stocks stockaction', '$ vz - why t-mobile bought verizons spectrum -> URL stock stocks stockaction', '$ vz us stocks-wall st ends flat on caution before u . s . jobs data URL video accumulationdistribution in excel URL doubletop $ v $ vz $ wmt $ xom $ sco $ sqqq $ agq $ qid $ fas $ tna $ iau $ gld $ ewa', '$ vz - messaging app market still has room for massive growth -> URL stock stocks stockaction $ vz will t-mobile really take over the wireless industry ? URL']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "hf_dataset = hf_dataset.map(preprocess_dataset, batched=True)\n",
    "print(hf_dataset)\n",
    "print(hf_dataset[0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 256])\n",
      "torch.Size([2, 4, 256])\n",
      "torch.Size([2, 4, 1])\n",
      "torch.Size([2, 4, 1])\n",
      "torch.Size([2, 4])\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = DataLoader(hf_dataset, batch_size=2, shuffle=True,collate_fn=custom_collate_fn)\n",
    "for ex in train_dataloader:\n",
    "    print(ex[\"input_ids\"].shape)\n",
    "    print(ex[\"attention_mask\"].shape)\n",
    "    print(ex[\"prices\"].shape)\n",
    "    print(ex[\"volumes\"].shape)\n",
    "    print(ex[\"labels\"].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m         result[\u001b[39m\"\u001b[39m\u001b[39mword_ids\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m [result\u001b[39m.\u001b[39mword_ids(i) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(result[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m]))]\n\u001b[1;32m      8\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n\u001b[0;32m----> 9\u001b[0m result \u001b[39m=\u001b[39m tokenize_function(dataset[\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m1\u001b[39m:\u001b[39m3\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# test the encode and decode sequence: done\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
    "def tokenize_function(examples):\n",
    "    lower = [preprocess(x) for x in examples[\"text\"]]\n",
    "    result = tokenizer(lower)\n",
    "    if tokenizer.is_fast:\n",
    "        result[\"word_ids\"] = [result.word_ids(i) for i in range(len(result[\"input_ids\"]))]\n",
    "    return result\n",
    "result = tokenize_function(dataset[\"train\"][1:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[0, 9713, 3263, 1215, 47955, 4819, 9, 2350, 18, 3748, 5182, 4246, 68, 10, 102, 2911, 68, 885, 3892, 282, 68, 213, 2154, 68, 784, 571, 506, 721, 2050, 3964, 1258, 1735, 17066, 3923, 6031, 27586, 918, 111, 111, 33000, 2], [0, 405, 705, 40, 2501, 15162, 33000, 68, 10, 102, 2911, 15162, 2]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'word_ids': [[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 8, 9, 10, 11, 11, 11, 12, 13, 13, 13, 14, 15, 15, 16, 17, 17, 17, 18, 18, 18, 18, 19, 20, 20, 20, 20, 20, 21, 22, 23, None], [None, 0, 0, 1, 2, 3, 4, 5, 6, 6, 6, 7, None]]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>rt @user summary of yesterday's webcast featuring $ aapl $ wynn $ goog $ lgf tradereducation options hedgingstrategies - - URL</s>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text': ['rt',\n",
       "  'AT_USER',\n",
       "  'summary',\n",
       "  'of',\n",
       "  \"yesterday's\",\n",
       "  'webcast',\n",
       "  'featuring',\n",
       "  '$',\n",
       "  'aapl',\n",
       "  '$',\n",
       "  'wynn',\n",
       "  '$',\n",
       "  'goog',\n",
       "  '$',\n",
       "  'lgf',\n",
       "  'tradereducation',\n",
       "  'options',\n",
       "  'hedgingstrategies',\n",
       "  '-',\n",
       "  '-',\n",
       "  'URL'],\n",
       " 'created_at': 'Wed Jan 01 03:29:29 +0000 2014',\n",
       " 'user_id_str': '1933063572'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tokenizer.decode(result[\"input_ids\"][0]))\n",
    "dataset[\"train\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
